{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chaitanya-rdY/I-am-besides-you/blob/main/I'M_Besides_You.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dd5bf9-3dfe-4375-a39d-27124355d912",
      "metadata": {
        "id": "07dd5bf9-3dfe-4375-a39d-27124355d912"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea71ad2-3de2-43c1-83bb-27230dbb802b",
      "metadata": {
        "id": "4ea71ad2-3de2-43c1-83bb-27230dbb802b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments,BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "093e76f6-a744-4f25-8e15-1e25f8556384",
      "metadata": {
        "id": "093e76f6-a744-4f25-8e15-1e25f8556384"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df  = pd.read_csv(\"test.csv\")\n",
        "\n",
        "request_types = [\n",
        "    \"Adjustment in Credits requirement for Graduation\",\n",
        "    \"Applying for B.Tech Degree with Honours\",\n",
        "    \"Applying for B.Tech Degree with Minor\",\n",
        "    \"Biometric Attendance â€“ In campus but not able to mark\",\n",
        "    \"Bonafide for Course Completion\",\n",
        "    \"Bonafide for Other Purposes\",\n",
        "    \"Bonafide for Passes ( Railway/Bus)\",\n",
        "    \"Bonafide for Passport\",\n",
        "    \"Bonafide for scholarship\",\n",
        "    \"Conference\",\n",
        "    \"Course Category Change Request\",\n",
        "    \"Course Registration Request\",\n",
        "    \"Course related Request ( other)\",\n",
        "    \"Course Withdraw Beyond Permissible Time\",\n",
        "    \"Credit limit request\",\n",
        "    \"Early Post Doc Fellowship\",\n",
        "    \"Extension of Ph.D HTRA to Female Scholars\",\n",
        "    \"Guest house booking request (On Paid basis)\",\n",
        "    \"ID CARD Request\",\n",
        "    \"Income certificate/Category change request\",\n",
        "    \"International Conference\",\n",
        "    \"LIBRARY CARD\",\n",
        "    \"Mess Fee relates Requests\",\n",
        "    \"Mess Related Requests\",\n",
        "    \"NOC for VISA\",\n",
        "    \"Other non academic request\",\n",
        "    \"Others\",\n",
        "    \"Permission for Leave/Absence from classes\",\n",
        "    \"Permission to stay on Campus during summer\",\n",
        "    \"Permission/NoC for internship\",\n",
        "    \"Ph.D Contingency\",\n",
        "    \"PhD Related Request\",\n",
        "    \"PhD Scholarship Request\",\n",
        "    \"Provisional Certificate\",\n",
        "    \"Request for UGTA stipend\",\n",
        "    \"Student Claim/Sanction Approval\",\n",
        "    \"Student Fee Related Request\",\n",
        "    \"Support for PhD students - foreign internship\",\n",
        "    \"Transcript Request\",\n",
        "    \"Transfer of Grades of Course Work completed from other Institutes\",\n",
        "    \"Withdrawal from admission\"\n",
        "]\n",
        "\n",
        "train_df[\"formatted\"] = \"Input: \" + train_df[\"text\"] + \"\\nOutput: \" + train_df[\"label\"]\n",
        "test_df[\"formatted\"]  = \"Input: \" + test_df[\"text\"] + \"\\nOutput: \" + test_df[\"label\"]\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df[[\"formatted\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"formatted\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82d99fb4-cc73-490c-8759-0c062f4ca286",
      "metadata": {
        "id": "82d99fb4-cc73-490c-8759-0c062f4ca286",
        "outputId": "5b48082a-08f9-44b8-9a23-9dfc686a6b34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: hf <command> [<args>]\n",
            "hf: error: argument {auth,cache,download,jobs,repo,repo-files,upload,upload-large-folder,env,version,lfs-enable-largefiles,lfs-multipart-upload}: invalid choice: 'login' (choose from 'auth', 'cache', 'download', 'jobs', 'repo', 'repo-files', 'upload', 'upload-large-folder', 'env', 'version', 'lfs-enable-largefiles', 'lfs-multipart-upload')\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m device_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      5\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhf_JlYJkkpIBbYqcmmeDJuCYGYHJdxAEKGiiU\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\models\\auto\\auto_factory.py:493\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    492\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:2842\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2838\u001b[0m         device_map_without_lm_head \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   2839\u001b[0m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[0;32m   2840\u001b[0m         }\n\u001b[0;32m   2841\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m-> 2842\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2843\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2844\u001b[0m \u001b[38;5;124;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[0;32m   2845\u001b[0m \u001b[38;5;124;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;124;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[0;32m   2847\u001b[0m \u001b[38;5;124;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[0;32m   2848\u001b[0m \u001b[38;5;124;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[0;32m   2849\u001b[0m \u001b[38;5;124;03m                for more details.\u001b[39;00m\n\u001b[0;32m   2850\u001b[0m \u001b[38;5;124;03m                \"\"\"\u001b[39;00m\n\u001b[0;32m   2851\u001b[0m             )\n\u001b[0;32m   2852\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[0;32m   2854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mValueError\u001b[0m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
          ]
        }
      ],
      "source": [
        "\n",
        "!hf login --token hf_JlYJkkpIBbYqcmmeDJuCYGYHJdxAEKGii\n",
        "MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "device_map = \"auto\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_auth_token=\"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\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "becbc910-be1b-4dc3-ba30-fed12a737579",
      "metadata": {
        "id": "becbc910-be1b-4dc3-ba30-fed12a737579"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df70c6b-4937-4d97-86f5-edd5c525a760",
      "metadata": {
        "id": "0df70c6b-4937-4d97-86f5-edd5c525a760"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./llama2-qlora-requests\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc70785-47a8-4810-97bc-c653cf4e9901",
      "metadata": {
        "id": "afc70785-47a8-4810-97bc-c653cf4e9901",
        "outputId": "3faf108a-7c0d-4f33-9bbb-b56164872772"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL_ID,\n\u001b[0;32m      3\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[0;32m      4\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_ds,\n\u001b[0;32m      5\u001b[0m     peft_config\u001b[38;5;241m=\u001b[39mpeft_config,\n\u001b[0;32m      6\u001b[0m     dataset_text_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m----> 8\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m,\n\u001b[0;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_arguments,\n\u001b[0;32m     10\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=MODEL_ID,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=256,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,\n",
        "    compute_metrics=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea28770d-67e2-49ab-a837-827bcb7a0157",
      "metadata": {
        "id": "ea28770d-67e2-49ab-a837-827bcb7a0157"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c344c8a5-d1d0-4d38-ad6c-8862e2f37215",
      "metadata": {
        "id": "c344c8a5-d1d0-4d38-ad6c-8862e2f37215"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    preds = preds.argmax(axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, preds),\n",
        "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b540252-cd32-4501-af01-08d877f168f3",
      "metadata": {
        "id": "3b540252-cd32-4501-af01-08d877f168f3"
      },
      "outputs": [],
      "source": [
        "def classify_text(example: str):\n",
        "    prompt = f\"Input: {example}\\nOutput:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    if \"Output:\" in prediction:\n",
        "        prediction = prediction.split(\"Output:\")[-1].strip()\n",
        "    return prediction\n",
        "\n",
        "sample = \"I want a certificate for my scholarship application\"\n",
        "print(\"Prediction:\", classify_text(sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7601435-e11b-43b3-b1a2-94e9796c5034",
      "metadata": {
        "id": "b7601435-e11b-43b3-b1a2-94e9796c5034"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5298a01-87e4-4c9c-95b0-796680dba749",
      "metadata": {
        "id": "c5298a01-87e4-4c9c-95b0-796680dba749"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}